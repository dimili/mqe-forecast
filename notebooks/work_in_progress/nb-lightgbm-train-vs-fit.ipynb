{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params ={\n",
    "      \"boosting_type\": \"gbdt\",\n",
    "      \"objective\": 'quantile',\n",
    "      \"num_trees\": 300,\n",
    "      \"learning_rate\": 0.2,\n",
    "      \"max_depth\": 8,\n",
    "      \"min_data_in_leaf\": 50,\n",
    "      \"max_leaves\": 128,\n",
    "      \"bagging_fraction\": 1,\n",
    "      \"bagging_freq\": 0,\n",
    "      \"feature_fraction\": 1,\n",
    "      \"lambda_l1\": 0.0,\n",
    "      \"lambda_l2\": 0.001,\n",
    "      \"min_child_weight\": 1e-3, \n",
    "      \"alpha\": 0.5,\n",
    "      \"max_bin\": 30,\n",
    "      \"n_jobs\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(objective='quantile',\n",
    "                          alpha=0.5,\n",
    "                          boosting_type=model_params.get('boosting_type', 'gbdt'),\n",
    "                          n_estimators=model_params.get('num_trees', 100),\n",
    "                          learning_rate=model_params.get('learning_rate', 0.1), \n",
    "                          max_depth=model_params.get('max_depth', -1), \n",
    "                          min_child_samples=model_params.get('min_data_in_leaf', 20), \n",
    "                          num_leaves=model_params.get('max_leaves', 31),\n",
    "                          subsample=model_params.get('bagging_fraction', 1.0), \n",
    "                          subsample_freq=model_params.get('bagging_freq', 0.0), \n",
    "                          colsample_bytree=model_params.get('feature_fraction', 1.0), \n",
    "                          reg_alpha=model_params.get('lambda_l1', 0.0), \n",
    "                          reg_lambda=model_params.get('lambda_l2', 0.0), \n",
    "                          min_sum_hessian_in_leaf=model_params.get(\"min_child_weightt\"),\n",
    "                          random_state=111,\n",
    "                          **{'max_bin': model_params['max_bin'], 'n_jobs': model_params['n_jobs']})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=200, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
    "X_train, y_train = X[:100], y[:100]\n",
    "X_test, y_test = X[100:], y[100:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "t0 = time.time()\n",
    "for _ in range(500):\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              sample_weight=None,\n",
    "              verbose=False,\n",
    "              callbacks=None)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_set = lightgbm.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "t0 = time.time()\n",
    "for _ in range(500):\n",
    "    gbm = lightgbm.train(model_params, train_set)\n",
    "print(time.time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(gbm.predict(X)-model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn API results\n",
      "2.437354803085327\n",
      "Native API results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/gbdt-forecast/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/anaconda3/envs/gbdt-forecast/lib/python3.8/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.773380994796753\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "\n",
    "# Generate Data Set\n",
    "xs = np.linspace(0, 10, 100).reshape((-1, 1)) \n",
    "ys = xs**2 + 4*xs + 5.2\n",
    "ys = ys.reshape((-1,))\n",
    "\n",
    "# Or you could add to your alg_conf \"min_child_weight\": 1e-3, \"min_child_samples\": 20.\n",
    "\n",
    "# LGBM configuration\n",
    "alg_conf = {\n",
    "    \"num_boost_round\":25,\n",
    "    \"max_depth\" : 3,\n",
    "    \"num_leaves\" : 31,\n",
    "    'learning_rate' : 0.1,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'objective' : 'regression_l2',\n",
    "    \"early_stopping_rounds\": None,\n",
    "    \"min_child_weight\": 1e-3, \n",
    "    \"min_child_samples\": 20\n",
    "}\n",
    "\n",
    "# Calling Regressor using scikit-learn API \n",
    "sk_reg = lgbm.sklearn.LGBMRegressor(\n",
    "    num_leaves=alg_conf[\"num_leaves\"], \n",
    "    n_estimators=alg_conf[\"num_boost_round\"], \n",
    "    max_depth=alg_conf[\"max_depth\"],\n",
    "    learning_rate=alg_conf[\"learning_rate\"],\n",
    "    objective=alg_conf[\"objective\"],\n",
    "    min_sum_hessian_in_leaf=alg_conf[\"min_child_weight\"],\n",
    "    min_data_in_leaf=alg_conf[\"min_child_samples\"]\n",
    ")\n",
    "\n",
    "print(\"Scikit-learn API results\")\n",
    "t0 = time.time()\n",
    "for _ in range(1000):\n",
    "    sk_reg.fit(xs, ys)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(\"Native API results\")\n",
    "# Calling Regressor using native API \n",
    "train_dataset = lgbm.Dataset(xs, ys)\n",
    "t0 = time.time()\n",
    "for _ in range(1000):\n",
    "    lg_reg = lgbm.train(alg_conf.copy(), train_dataset)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(lg_reg.predict(xs)-sk_reg.predict(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gbdt-forecast]",
   "language": "python",
   "name": "conda-env-gbdt-forecast-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
